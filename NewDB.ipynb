{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "668199e3-e8cf-4205-a91b-75240fb41f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Hello World\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started\n",
      "Loading CSV...\n",
      "CSV loaded. Splitting train/test...\n",
      "Train/test split done. Preparing training samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 48635 training samples. Building vector store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bl/7hs9fb8x1zj_sb_t855mfk8c0000gn/T/ipykernel_7423/1727306738.py:41: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embeddings = HuggingFaceBgeEmbeddings(\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: BAAI/bge-small-en-v1.5\n",
      "/var/folders/bl/7hs9fb8x1zj_sb_t855mfk8c0000gn/T/ipykernel_7423/1727306738.py:56: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  self.vector_store = Chroma(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 290\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrepared \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(samples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m training samples. Building vector store...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# Build vector store with training data\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m store\u001b[38;5;241m=\u001b[39m DatasetVectorStore()\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVector store created. Adding documents...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# save embedding data\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 56\u001b[0m, in \u001b[0;36mDatasetVectorStore.__init__\u001b[0;34m(self, db_name, collection_name, persist_directory)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m PersistentClient(\n\u001b[1;32m     52\u001b[0m     path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpersist_directory, settings\u001b[38;5;241m=\u001b[39mSettings(anonymized_telemetry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# keep client information privately\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_store \u001b[38;5;241m=\u001b[39m Chroma(\n\u001b[1;32m     57\u001b[0m     client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient,\n\u001b[1;32m     58\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollection_name,\n\u001b[1;32m     59\u001b[0m     embedding_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings,\n\u001b[1;32m     60\u001b[0m     persist_directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpersist_directory,\n\u001b[1;32m     61\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:222\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     emit_warning()\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/langchain_community/vectorstores/chroma.py:128\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persist_directory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    124\u001b[0m         _client_settings\u001b[38;5;241m.\u001b[39mpersist_directory \u001b[38;5;129;01mor\u001b[39;00m persist_directory\n\u001b[1;32m    125\u001b[0m     )\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;241m=\u001b[39m embedding_function\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mget_or_create_collection(\n\u001b[1;32m    129\u001b[0m     name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[1;32m    130\u001b[0m     embedding_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    131\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mcollection_metadata,\n\u001b[1;32m    132\u001b[0m )\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverride_relevance_score_fn \u001b[38;5;241m=\u001b[39m relevance_score_fn\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/chromadb/api/client.py:194\u001b[0m, in \u001b[0;36mClient.get_or_create_collection\u001b[0;34m(self, name, configuration, metadata, embedding_function, data_loader)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_or_create_collection\u001b[39m(\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m     data_loader: Optional[DataLoader[Loadable]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    193\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Collection:\n\u001b[0;32m--> 194\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server\u001b[38;5;241m.\u001b[39mget_or_create_collection(\n\u001b[1;32m    195\u001b[0m         name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    196\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[1;32m    197\u001b[0m         tenant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtenant,\n\u001b[1;32m    198\u001b[0m         database\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatabase,\n\u001b[1;32m    199\u001b[0m         configuration\u001b[38;5;241m=\u001b[39mconfiguration,\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Collection(\n\u001b[1;32m    202\u001b[0m         client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server,\n\u001b[1;32m    203\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    204\u001b[0m         embedding_function\u001b[38;5;241m=\u001b[39membedding_function,\n\u001b[1;32m    205\u001b[0m         data_loader\u001b[38;5;241m=\u001b[39mdata_loader,\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/chromadb/telemetry/opentelemetry/__init__.py:150\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/chromadb/api/segment.py:103\u001b[0m, in \u001b[0;36mrate_limit.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rate_limit_enforcer\u001b[38;5;241m.\u001b[39mrate_limit(func)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/chromadb/rate_limit/simple_rate_limit/__init__.py:23\u001b[0m, in \u001b[0;36mSimpleRateLimitEnforcer.rate_limit.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/chromadb/api/segment.py:272\u001b[0m, in \u001b[0;36mSegmentAPI.get_or_create_collection\u001b[0;34m(self, name, configuration, metadata, tenant, database)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;129m@trace_method\u001b[39m(\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSegmentAPI.get_or_create_collection\u001b[39m\u001b[38;5;124m\"\u001b[39m, OpenTelemetryGranularity\u001b[38;5;241m.\u001b[39mOPERATION\n\u001b[1;32m    261\u001b[0m )\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    270\u001b[0m     database: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m DEFAULT_DATABASE,\n\u001b[1;32m    271\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CollectionModel:\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_collection(\n\u001b[1;32m    273\u001b[0m         name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    274\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[1;32m    275\u001b[0m         configuration\u001b[38;5;241m=\u001b[39mconfiguration,\n\u001b[1;32m    276\u001b[0m         get_or_create\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    277\u001b[0m         tenant\u001b[38;5;241m=\u001b[39mtenant,\n\u001b[1;32m    278\u001b[0m         database\u001b[38;5;241m=\u001b[39mdatabase,\n\u001b[1;32m    279\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/chromadb/telemetry/opentelemetry/__init__.py:150\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/chromadb/api/segment.py:103\u001b[0m, in \u001b[0;36mrate_limit.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rate_limit_enforcer\u001b[38;5;241m.\u001b[39mrate_limit(func)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/chromadb/rate_limit/simple_rate_limit/__init__.py:23\u001b[0m, in \u001b[0;36mSimpleRateLimitEnforcer.rate_limit.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/chromadb/api/segment.py:226\u001b[0m, in \u001b[0;36mSegmentAPI.create_collection\u001b[0;34m(self, name, configuration, metadata, get_or_create, tenant, database)\u001b[0m\n\u001b[1;32m    213\u001b[0m model \u001b[38;5;241m=\u001b[39m CollectionModel(\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m,\n\u001b[1;32m    215\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m     dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    223\u001b[0m )\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# TODO: Let sysdb create the collection directly from the model\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m coll, created \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sysdb\u001b[38;5;241m.\u001b[39mcreate_collection(\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mid,\n\u001b[1;32m    228\u001b[0m     name\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    229\u001b[0m     configuration\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_configuration(),\n\u001b[1;32m    230\u001b[0m     segments\u001b[38;5;241m=\u001b[39m[], \u001b[38;5;66;03m# Passing empty till backend changes are deployed.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmetadata,\n\u001b[1;32m    232\u001b[0m     dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# This is lazily populated on the first add\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     get_or_create\u001b[38;5;241m=\u001b[39mget_or_create,\n\u001b[1;32m    234\u001b[0m     tenant\u001b[38;5;241m=\u001b[39mtenant,\n\u001b[1;32m    235\u001b[0m     database\u001b[38;5;241m=\u001b[39mdatabase,\n\u001b[1;32m    236\u001b[0m )\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created:\n\u001b[1;32m    239\u001b[0m     segments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39mprepare_segments_for_new_collection(coll)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/chromadb/telemetry/opentelemetry/__init__.py:150\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/chromadb/db/mixins/sysdb.py:230\u001b[0m, in \u001b[0;36mSqlSysDB.create_collection\u001b[0;34m(self, id, name, configuration, segments, metadata, dimension, get_or_create, tenant, database)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid must be specified if get_or_create is False\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    223\u001b[0m add_attributes_to_current_span(\n\u001b[1;32m    224\u001b[0m     {\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollection_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m),\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollection_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: name,\n\u001b[1;32m    227\u001b[0m     }\n\u001b[1;32m    228\u001b[0m )\n\u001b[0;32m--> 230\u001b[0m existing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_collections(name\u001b[38;5;241m=\u001b[39mname, tenant\u001b[38;5;241m=\u001b[39mtenant, database\u001b[38;5;241m=\u001b[39mdatabase)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m existing:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m get_or_create:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/chromadb/telemetry/opentelemetry/__init__.py:150\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/chromadb/db/mixins/sysdb.py:456\u001b[0m, in \u001b[0;36mSqlSysDB.get_collections\u001b[0;34m(self, id, name, tenant, database, limit, offset)\u001b[0m\n\u001b[1;32m    454\u001b[0m dimension \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(rows[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m3\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m rows[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rows[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 456\u001b[0m     configuration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_config_from_json_str_and_migrate(\n\u001b[1;32m    457\u001b[0m         \u001b[38;5;28mstr\u001b[39m(collection_id), rows[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    458\u001b[0m     )\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# 07/2024: This is a legacy case where we don't have a collection\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# configuration stored in the database. This non-destructively migrates\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# the collection to have a configuration, and takes into account any\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;66;03m# HNSW params that might be in the existing metadata.\u001b[39;00m\n\u001b[1;32m    464\u001b[0m     configuration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insert_config_from_legacy_params(\n\u001b[1;32m    465\u001b[0m         collection_id, metadata\n\u001b[1;32m    466\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/chromadb/db/mixins/sysdb.py:807\u001b[0m, in \u001b[0;36mSqlSysDB._load_config_from_json_str_and_migrate\u001b[0;34m(self, collection_id, json_str)\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    803\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to decode configuration from JSON string: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    804\u001b[0m     )\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CollectionConfigurationInternal\u001b[38;5;241m.\u001b[39mfrom_json_str(json_str)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidConfigurationError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;66;03m# 07/17/2024: the initial migration from the legacy metadata-based config to the new sysdb-based config had a bug where the batch_size and sync_threshold were swapped. Along with this migration, a validator was added to HNSWConfigurationInternal to ensure that batch_size <= sync_threshold.\u001b[39;00m\n\u001b[1;32m    810\u001b[0m     hnsw_configuration \u001b[38;5;241m=\u001b[39m config_json\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhnsw_configuration\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/chromadb/api/configuration.py:188\u001b[0m, in \u001b[0;36mConfigurationInternal.from_json_str\u001b[0;34m(cls, json_str)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to decode configuration from JSON string: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    187\u001b[0m     )\n\u001b[0;32m--> 188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_json(config_json)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/chromadb/api/configuration.py:209\u001b[0m, in \u001b[0;36mConfigurationInternal.from_json\u001b[0;34m(cls, json_map)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a configuration from the given JSON string.\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m json_map\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m--> 209\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to instantiate configuration of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from JSON with type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_map[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    210\u001b[0m     )\n\u001b[1;32m    211\u001b[0m parameters \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m json_map\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# Type value is only for storage\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: '_type'"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "from typing import List\n",
    "from uuid import uuid4\n",
    "\n",
    "from langchain.schema import Document\n",
    "from chromadb import PersistentClient\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from chromadb.config import Settings\n",
    "import logging \n",
    "for handler in logging.root.handlers [:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "    \n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "logger.info(\"Hello World\")\n",
    "\n",
    "\n",
    "class DatasetVectorStore:\n",
    "    \"\"\"ChromaDB vector store for PublicationModel objects with SentenceTransformers embeddings.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        db_name: str = \"retrieval_augmented_classification\",  # Using db_name as collection name in Chroma\n",
    "        collection_name: str = \"classification_dataset\",\n",
    "        persist_directory: str = \"chroma_db\",  # Directory to persist ChromaDB\n",
    "    ):\n",
    "        self.db_name = db_name\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "\n",
    "        # Determine if CUDA is available\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        logger.info(f\"Using device: {device}\")\n",
    "\n",
    "        self.embeddings = HuggingFaceBgeEmbeddings(\n",
    "            model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "            model_kwargs={\"device\": device},\n",
    "            encode_kwargs={\n",
    "                \"device\": device,\n",
    "                \"batch_size\": 100,\n",
    "            },  # Adjust batch_size as needed\n",
    "        )\n",
    "\n",
    "        # Initialize Chroma vector store\n",
    "        self.client = PersistentClient(\n",
    "            path=self.persist_directory, settings=Settings(anonymized_telemetry=False)\n",
    "        )\n",
    "        # keep client information privately\n",
    "        \n",
    "        self.vector_store = Chroma(\n",
    "            client=self.client,\n",
    "            collection_name=self.collection_name,\n",
    "            embedding_function=self.embeddings,\n",
    "            persist_directory=self.persist_directory,\n",
    "        )\n",
    "\n",
    "    def add_documents(self, documents: List) -> None:\n",
    "        \"\"\"\n",
    "        Add multiple documents to the vector store.\n",
    "\n",
    "        Args:\n",
    "            documents: List of dictionaries containing document data.  Each dict needs a \"text\" key.\n",
    "        \"\"\"\n",
    "\n",
    "        local_documents = []\n",
    "        ids = []\n",
    "        # make empty list for documents saving\n",
    "\n",
    "        for doc_data in documents:\n",
    "            if not doc_data.get(\"id\"):\n",
    "                doc_data[\"id\"] = str(uuid4())\n",
    "\n",
    "            # uuind 4 : make a unique id for exmaple: {\"text\":\"I am trying to learn LLMs\",\"category\":\"training\", \"id\": \"a1b2c3...\"}\n",
    "\n",
    "            local_documents.append(\n",
    "                Document(\n",
    "                    page_content=doc_data[\"text\"],\n",
    "                    metadata={k: v for k, v in doc_data.items() if k != \"text\"},\n",
    "                )\n",
    "            )\n",
    "            ids.append(doc_data[\"id\"])\n",
    "\n",
    "        batch_size = 100  # Adjust batch size as needed\n",
    "        for i in tqdm(range(0, len(documents), batch_size)):\n",
    "            batch_docs = local_documents[i : i + batch_size]\n",
    "            batch_ids = ids[i : i + batch_size]\n",
    "\n",
    "            # Chroma's add_documents doesn't directly support pre-defined IDs. Upsert instead.\n",
    "            self._upsert_batch(batch_docs, batch_ids)\n",
    "\n",
    "    def _upsert_batch(self, batch_docs: List[Document], batch_ids: List[str]):\n",
    "        \"\"\"Upsert a batch of documents into Chroma.  If the ID exists, it updates; otherwise, it creates.\"\"\"\n",
    "        texts = [doc.page_content for doc in batch_docs]\n",
    "        metadatas = [doc.metadata for doc in batch_docs]\n",
    "\n",
    "        self.vector_store.add_texts(texts=texts, metadatas=metadatas, ids=batch_ids)\n",
    "\n",
    "    def search(self, query: str, k: int = 5) -> List[Document]:\n",
    "        \"\"\"Search documents by semantic similarity.\"\"\"\n",
    "        results = self.vector_store.similarity_search(query, k=3)\n",
    "        return results\n",
    "\n",
    "# it work based on semantic search (jostojoo bar asaas ma'na)\n",
    "# __init__ Function: \n",
    "# 1. It sets the name of the database and the path where the data will be saved.\n",
    "# 2. Then it checks if the system is using CPU or GPU. (GPU is faster for processing.)\n",
    "# 3. Next, it loads an embedding model that can turn text into numbers.\n",
    "# 4. This model always turns any sentence into a vector with 384 numbers.\n",
    "# 5. Finally, it creates a ChromaDB vector database, so that later, it can store the text and its vector together.\n",
    "# __add__ _documents Function: \n",
    "# 1. If the text doesn't have an ID, it creates a unique ID for it.\n",
    "# 2. It prepares the text and its info (like category).\n",
    "# 3. It changes the text into numbers (a vector).\n",
    "# 4. It saves everything into the Chroma database.\n",
    "# __upsert__ _batch Function: update + insert\n",
    "#\n",
    "# This function takes a batch of documents and their IDs, and stores them in ChromaDB.\n",
    "#\n",
    "# Steps:\n",
    "# 1. It separates the actual text from each document into a 'texts' list.\n",
    "# 2. It collects the extra info (like category and id) into a 'metadatas' list.\n",
    "# 3. It also creates a separate 'ids' list, even though IDs are already in the metadata.\n",
    "#\n",
    "# Why do we separate the IDs?\n",
    "# - ChromaDB requires 'ids' to be passed separately.\n",
    "# - It uses them to know whether to insert or update a document (upsert).\n",
    "# - It also uses the ID as a unique key for searching or managing documents.\n",
    "#\n",
    "# Even if the ID is inside the metadata, Chroma doesn't use it unless it's passed directly in the 'ids' list.\n",
    "#\n",
    "# Only the 'text' is turned into a vector (embedding). The 'metadata' is just stored along with it for filtering or reference.\n",
    "# store = DatasetVectorStore()\n",
    "# RAC\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from collections import Counter\n",
    "\n",
    "#from retrieval_augmented_classification.vector_store import DatasetVectorStore (we dont need this command, because all codes are together)\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "class PredictedCategories(BaseModel):\n",
    "    \"\"\"\n",
    "    Pydantic model for the predicted categories from the LLM.\n",
    "    \"\"\"\n",
    "\n",
    "    reasoning: str = Field(description=\"Explain your reasoning\")\n",
    "    predicted_category: str = Field(description=\"Category\")\n",
    "# This class defines the format of the model's answer: \n",
    "# It must return a reason and a predicted category.\n",
    "\n",
    "class RAC:\n",
    "    \"\"\"\n",
    "    A hybrid classifier combining K-Nearest Neighbors retrieval with an LLM for multi-class prediction.\n",
    "    Finds top K neighbors, uses top few-shot for context, and uses all neighbor categories\n",
    "    as potential prediction candidates for the LLM.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: DatasetVectorStore,\n",
    "        llm_client,\n",
    "        knn_k_search: int = 30,\n",
    "        knn_k_few_shot: int = 5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the classifier.\n",
    "\n",
    "        Args:\n",
    "            vector_store: An instance of DatasetVectorStore with a search method.\n",
    "            llm_client: An instance of the LLM client capable of structured output.\n",
    "            knn_k_search: The number of nearest neighbors to retrieve from the vector store.\n",
    "            knn_k_few_shot: The number of top neighbors to use as few-shot examples for the LLM.\n",
    "                           Must be less than or equal to knn_k_search.\n",
    "        \"\"\"\n",
    "\n",
    "        self.vector_store = vector_store\n",
    "        self.llm_client = llm_client\n",
    "        self.knn_k_search = knn_k_search\n",
    "        self.knn_k_few_shot = knn_k_few_shot\n",
    "\n",
    "    # @retry(\n",
    "    #     stop=stop_after_attempt(3),  # Retry LLM call a few times\n",
    "    #     # wait=wait_exponential(multiplier=1, min=2, max=5),  # Shorter waits for demo\n",
    "    # )\n",
    "    def predict(self, document_text: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Predicts the relevant categories for a given document text using KNN retrieval and an LLM.\n",
    "\n",
    "        Args:\n",
    "            document_text: The text content of the document to classify.\n",
    "\n",
    "        Returns:\n",
    "            The predicted category\n",
    "        \"\"\"\n",
    "        neighbors = self.vector_store.search(document_text, k=self.knn_k_search)\n",
    "\n",
    "        all_neighbor_categories = set()\n",
    "        valid_neighbors = []  # Store neighbors that have metadata and categories\n",
    "        for neighbor in neighbors:\n",
    "            if (\n",
    "                hasattr(neighbor, \"metadata\")\n",
    "                and isinstance(neighbor.metadata, dict)\n",
    "                and \"category\" in neighbor.metadata\n",
    "            ):\n",
    "                all_neighbor_categories.add(neighbor.metadata[\"category\"])\n",
    "                valid_neighbors.append(neighbor)\n",
    "            else:\n",
    "                pass  # Suppress warnings for cleaner demo output\n",
    "\n",
    "        if not valid_neighbors:\n",
    "            return None\n",
    "\n",
    "        category_counts = Counter(all_neighbor_categories)\n",
    "        ranked_categories = [\n",
    "            category for category, count in category_counts.most_common()\n",
    "        ]\n",
    "\n",
    "        if not ranked_categories:\n",
    "            return None\n",
    "\n",
    "        few_shot_neighbors = valid_neighbors[: self.knn_k_few_shot]\n",
    "\n",
    "        messages = []\n",
    "\n",
    "        system_prompt = f\"\"\"You are an expert multi-class classifier. Your task is to analyze the provided document text and assign the most relevant category from the list of allowed categories.\n",
    "You MUST only return categories that are present in the following list: {ranked_categories}.\n",
    "If none of the allowed categories are relevant, return an empty list.\n",
    "Return the categories by likelihood (more confident to least confident).\n",
    "Output your prediction as a JSON object matching the Pydantic schema: {PredictedCategories.model_json_schema()}.\n",
    "\"\"\"\n",
    "        messages.append(SystemMessage(content=system_prompt))\n",
    "\n",
    "        for i, neighbor in enumerate(few_shot_neighbors):\n",
    "            messages.append(\n",
    "                HumanMessage(content=f\"Document: {neighbor.page_content}\")\n",
    "            )\n",
    "            expected_output_json = PredictedCategories(\n",
    "                reasoning=\"Your reasoning here\",\n",
    "                predicted_category=neighbor.metadata[\"category\"]\n",
    "            ).model_dump_json()\n",
    "            # Simulate the structure often used with tool calling/structured output\n",
    "\n",
    "            ai_message_with_tool = AIMessage(\n",
    "                content=expected_output_json,\n",
    "            )\n",
    "\n",
    "            messages.append(ai_message_with_tool)\n",
    "\n",
    "        # Final user message: The document text to classify\n",
    "        messages.append(HumanMessage(content=f\"Document: {document_text}\"))\n",
    "\n",
    "        # Configure the client for structured output with the Pydantic schema\n",
    "        structured_client = self.llm_client.with_structured_output(PredictedCategories)\n",
    "        llm_response: PredictedCategories = structured_client.invoke(messages)\n",
    "\n",
    "        predicted_category = llm_response.predicted_category\n",
    "\n",
    "        return predicted_category if predicted_category in ranked_categories else None\n",
    "\n",
    "# * @retry: it is a decorator : automatically retries the predict() function up to 3 times\n",
    "# if the LLM call fails (e.g., due to a temporary error or timeout).\n",
    "# Test\n",
    "# KNN\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Script started\")\n",
    "# Load CSV\n",
    "print(\"Loading CSV...\")\n",
    "df = pd.read_csv(\"DBPEDIA_test 5.csv\")\n",
    "print(\"CSV loaded. Splitting train/test...\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "print(\"Train/test split done. Preparing training samples...\")\n",
    "#Prepare training samples\n",
    "samples = [\n",
    "    {\"text\": row[\"text\"], \"category\": row[\"l3\"]}\n",
    "    for _, row in train_df.iterrows()\n",
    "]\n",
    "print(f\"Prepared {len(samples)} training samples. Building vector store...\")\n",
    "# Build vector store with training data\n",
    "store= DatasetVectorStore()\n",
    "print(\"Vector store created. Adding documents...\")\n",
    "# save embedding data\n",
    "store.add_documents(samples)\n",
    "print(\"Documents added to vector store. Defining KNN prediction function...\")\n",
    "# Define KNN prediction function\n",
    "\n",
    "\n",
    "def knn_predict(vector_store, text, k=5):\n",
    "    results = vector_store.search(text, k=k)\n",
    "    \n",
    "    categories = []\n",
    "    for doc in results:\n",
    "        if \"category\" in doc.metadata:\n",
    "            categories.append(doc.metadata[\"category\"])\n",
    "    \n",
    "    if not categories:\n",
    "        return None\n",
    "    \n",
    "    most_common_category = Counter(categories).most_common(1)[0][0]\n",
    "    return most_common_category\n",
    "\n",
    "# if in output 1/1 shows it meand 1 batch, we define batchs in 100 samples, so it means model test 100 samples.\n",
    "#in this case shows 13 batch\n",
    "\n",
    "#Accuracy\n",
    "from sklearn.metrics import accuracy_score,precision_score,f1_score,confusion_matrix\n",
    "\n",
    "print(\"Preparing test data...\")\n",
    "# Prepare test data\n",
    "test_texts = test_df[\"text\"].tolist()\n",
    "true_labels = test_df[\"l3\"].tolist()\n",
    "print(f\"Test data prepared: {len(test_texts)} samples. Running KNN predictions...\")\n",
    "# Predict using knn\n",
    "predicted_labels = []\n",
    "\n",
    "for text in test_texts:\n",
    "    pred = knn_predict(store, text, k=5)\n",
    "    predicted_labels.append(pred)\n",
    "print(\"KNN predictions done. Calculating accuracy...\")\n",
    "\n",
    "\n",
    "precision = precision_score(true_labels, predicted_labels, average='macro')\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, average='macro')\n",
    "f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "print(f\"KNN Accuracy: {accuracy:.2f}\")\n",
    "print(f\"KNN precision: {precision:.2f}\")\n",
    "print(f\"KNN f1: {f1:.2f}\")\n",
    "\n",
    "# ! pip install langchain transformers torch pydantic chromadb tqdm\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"Setting up LLM pipeline...\")\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\",\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    ")\n",
    "print(\"LLM pipeline ready. Running LLM-only predictions...\")\n",
    "def llm_only_predict(text: str, candidate_labels: List[str]) -> Optional[str]:\n",
    "    result = classifier(sequences=text, candidate_labels=candidate_labels)\n",
    "    return result[\"labels\"][0] if result and \"labels\" in result else None\n",
    "# تمام دسته‌ها (یونیک) از دیتافریم\n",
    "all_categories = sorted(df[\"l3\"].unique().tolist())\n",
    "\n",
    "# پیش‌بینی برای هر متن در تست‌ست با LLM\n",
    "test_texts = test_df[\"text\"].tolist()\n",
    "true_labels = test_df[\"l3\"].tolist()\n",
    "\n",
    "llm_preds = [llm_only_predict(text, all_categories) for text in test_texts]\n",
    "print(\"LLM-only predictions done. Calculating metrics...\")\n",
    "\n",
    "accuracy = accuracy_score(true_labels, llm_preds)\n",
    "precision = precision_score(true_labels, llm_preds, average='macro')\n",
    "f1 = f1_score(true_labels, llm_preds, average='macro')\n",
    "\n",
    "print(f\"LLM-only Accuracy: {accuracy:.4f}\")\n",
    "print(f\"LLM-only Precision: {precision:.4f}\")\n",
    "print(f\"LLM-only F1 Score: {f1:.4f}\")\n",
    "\n",
    "class RAC:\n",
    "    def __init__(self, vector_store, classifier, knn_k_search=20, knn_k_few_shot=5):\n",
    "        self.vector_store = vector_store\n",
    "        self.classifier = classifier\n",
    "        self.knn_k_search = knn_k_search\n",
    "        self.knn_k_few_shot = knn_k_few_shot\n",
    "\n",
    "    def predict(self, document_text: str) -> Optional[str]:\n",
    "        neighbors = self.vector_store.search(document_text, k=self.knn_k_search)\n",
    "\n",
    "        valid_neighbors = []\n",
    "        categories = []\n",
    "\n",
    "        for neighbor in neighbors:\n",
    "            if hasattr(neighbor, \"metadata\") and isinstance(neighbor.metadata, dict) and \"category\" in neighbor.metadata:\n",
    "                categories.append(neighbor.metadata[\"category\"])\n",
    "                valid_neighbors.append(neighbor)\n",
    "\n",
    "        if not valid_neighbors:\n",
    "            return None\n",
    "\n",
    "        # فقط دسته‌های یکتا\n",
    "        unique_categories = list(set(categories))\n",
    "\n",
    "        # نمونه‌های few-shot\n",
    "        few_shot_neighbors = valid_neighbors[: self.knn_k_few_shot]\n",
    "\n",
    "        # ساختن prompt: اول مثال‌ها، بعد سوال\n",
    "        prompt_parts = [\"You are a text classifier. Given a document, choose the most suitable category from the list provided.\\n\"]\n",
    "\n",
    "        for i, neighbor in enumerate(few_shot_neighbors):\n",
    "            prompt_parts.append(f\"Example {i+1}:\\nText: {neighbor.page_content}\\nCategory: {neighbor.metadata['category']}\\n\")\n",
    "\n",
    "        prompt_parts.append(f\"Now classify this document:\\nText: {document_text}\")\n",
    "\n",
    "        full_prompt = \"\\n\".join(prompt_parts)\n",
    "\n",
    "        # اجرای مدل BART-MNLI با candidate labels از همسایه‌ها\n",
    "        result = self.classifier(\n",
    "            sequences=full_prompt,\n",
    "            candidate_labels=unique_categories,\n",
    "        )\n",
    "\n",
    "        return result[\"labels\"][0] if result and \"labels\" in result else None\n",
    "print(\"Setting up RAC hybrid model...\")\n",
    "rac = RAC(vector_store=store, classifier=classifier, knn_k_search=20, knn_k_few_shot=5)\n",
    "print(\"RAC model ready. Running RAC predictions...\")\n",
    "predicted_labels = [rac.predict(text) for text in test_texts]\n",
    "print(\"RAC predictions done. Calculating metrics...\")\n",
    "\n",
    "# Accuracy metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, average='macro')\n",
    "f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "print(f\"RAC Accuracy: {accuracy:.2f}\")\n",
    "print(f\"RAC Precision: {precision:.2f}\")\n",
    "print(f\"RAC F1: {f1:.2f}\")\n",
    "# In your knn_predict or RAC predict\n",
    "# print(f\"Query: {text}\")\n",
    "# for doc in results:\n",
    "#     print(f\"Retrieved: {doc.page_content[:100]}... -> {doc.metadata['category']}\")\n",
    "# test by new data\n",
    "\n",
    "\n",
    "new_text_1= \" my favorite field to study is creating and visulizing sports images\"\n",
    "new_text_2 = \"Cristiano Ronaldo scored a goal for Portugal\"\n",
    "\n",
    "\n",
    "\n",
    "knn_category = knn_predict(store, new_text_1, k=5)\n",
    "print(\"KNN predicted category:\", knn_category)\n",
    "\n",
    "rac_category = rac.predict(new_text_1)\n",
    "print(\"RAC predicted category:\", rac_category)\n",
    "knn_category = knn_predict(store, new_text_2, k=5)\n",
    "print(\"KNN predicted category:\", knn_category)\n",
    "\n",
    "rac_category = rac.predict(new_text_2)\n",
    "print(\"RAC predicted category:\", rac_category)\n",
    "# Sources:\n",
    "# https://docs.google.com/spreadsheets/d/11WD6SiPNaVIQTzbLkU0K0DmDHYN7GLfOVcj3ap3-MdY/edit?pli=1&gid=0#gid=0\n",
    "# https://microsoft.github.io/generative-ai-for-beginners/#/\n",
    "# https://github.com/CVxTz/retrieval_augmented_classification\n",
    "class RAC:\n",
    "    def __init__(self, vector_store, classifier, predicted_labels):\n",
    "        self.vector_store = vector_store\n",
    "        self.classifier = classifier\n",
    "        self.knn_k_search = predicted_labels\n",
    "\n",
    "    def predict(self, document_text: str) -> Optional[str]:\n",
    "        neighbors = self.vector_store.search(document_text, k=self.knn_k_search)\n",
    "\n",
    "        # Collect categories from neighbors\n",
    "        categories = []\n",
    "        for neighbor in neighbors:\n",
    "            if (\n",
    "                hasattr(neighbor, \"metadata\")\n",
    "                and isinstance(neighbor.metadata, dict)\n",
    "                and \"category\" in neighbor.metadata\n",
    "            ):\n",
    "                categories.append(neighbor.metadata[\"category\"])\n",
    "\n",
    "        if not categories:\n",
    "            return None\n",
    "\n",
    "        # Deduplicate\n",
    "        unique_categories = list(set(categories))\n",
    "\n",
    "        # Use HF pipeline to classify\n",
    "        result = self.classifier(\n",
    "            sequences=document_text,\n",
    "            candidate_labels=unique_categories,\n",
    "        )\n",
    "\n",
    "        # Return the top predicted label\n",
    "        return result[\"labels\"][0] if result and \"labels\" in result else None\n",
    "\n",
    "true_labels = test_df[\"l3\"].tolist()\n",
    "test_texts = test_df[\"text\"].tolist()\n",
    "\n",
    "\n",
    "predicted_labels = [rac.predict(text) for text in test_texts]\n",
    "\n",
    "precision = precision_score(true_labels, predicted_labels, average='macro')\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, average='macro')\n",
    "f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "print(f\"RAC Accuracy: {accuracy:.2f}\")\n",
    "print(f\"RAC precision: {precision:.2f}\")\n",
    "print(f\"RAC f1: {f1:.2f}\")\n",
    "\n",
    "# --- Visualization and Comparison ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Collect metrics for each method\n",
    "methods = ['KNN', 'LLM', 'RAC']\n",
    "accuracies = []\n",
    "precisions = []\n",
    "f1s = []\n",
    "\n",
    "# KNN metrics (already computed above)\n",
    "accuracies.append(accuracy_score(true_labels, [knn_predict(store, t, k=5) for t in test_texts]))\n",
    "precisions.append(precision_score(true_labels, [knn_predict(store, t, k=5) for t in test_texts], average='macro'))\n",
    "f1s.append(f1_score(true_labels, [knn_predict(store, t, k=5) for t in test_texts], average='macro'))\n",
    "\n",
    "# LLM metrics (already computed above)\n",
    "llm_preds = [llm_only_predict(text, all_categories) for text in test_texts]\n",
    "accuracies.append(accuracy_score(true_labels, llm_preds))\n",
    "precisions.append(precision_score(true_labels, llm_preds, average='macro'))\n",
    "f1s.append(f1_score(true_labels, llm_preds, average='macro'))\n",
    "\n",
    "# RAC metrics (already computed above)\n",
    "rac_preds = [rac.predict(text) for text in test_texts]\n",
    "accuracies.append(accuracy_score(true_labels, rac_preds))\n",
    "precisions.append(precision_score(true_labels, rac_preds, average='macro'))\n",
    "f1s.append(f1_score(true_labels, rac_preds, average='macro'))\n",
    "\n",
    "# Plotting\n",
    "x = range(len(methods))\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar([i-0.2 for i in x], accuracies, width=0.2, label='Accuracy')\n",
    "plt.bar(x, precisions, width=0.2, label='Precision')\n",
    "plt.bar([i+0.2 for i in x], f1s, width=0.2, label='F1 Score')\n",
    "plt.xticks(x, methods)\n",
    "plt.ylabel('Score')\n",
    "plt.title('Comparison of KNN, LLM, and RAC')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Metrics and Confusion Matrices ---\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "methods = ['KNN', 'LLM', 'RAC']\n",
    "all_preds = []\n",
    "all_metrics = []\n",
    "all_cms = []\n",
    "\n",
    "# KNN\n",
    "knn_preds = [knn_predict(store, t, k=5) for t in test_texts]\n",
    "all_preds.append(knn_preds)\n",
    "knn_acc = accuracy_score(true_labels, knn_preds)\n",
    "knn_prec = precision_score(true_labels, knn_preds, average='macro', zero_division=0)\n",
    "knn_f1 = f1_score(true_labels, knn_preds, average='macro', zero_division=0)\n",
    "knn_cm = confusion_matrix(true_labels, knn_preds)\n",
    "all_metrics.append((knn_acc, knn_prec, knn_f1))\n",
    "all_cms.append(knn_cm)\n",
    "print(\"\\nKNN Results:\")\n",
    "print(f\"Accuracy: {knn_acc:.2f}\")\n",
    "print(f\"Precision: {knn_prec:.2f}\")\n",
    "print(f\"F1: {knn_f1:.2f}\")\n",
    "print(\"Confusion Matrix:\\n\", knn_cm)\n",
    "\n",
    "# LLM\n",
    "llm_preds = [llm_only_predict(text, all_categories) for text in test_texts]\n",
    "all_preds.append(llm_preds)\n",
    "llm_acc = accuracy_score(true_labels, llm_preds)\n",
    "llm_prec = precision_score(true_labels, llm_preds, average='macro', zero_division=0)\n",
    "llm_f1 = f1_score(true_labels, llm_preds, average='macro', zero_division=0)\n",
    "llm_cm = confusion_matrix(true_labels, llm_preds)\n",
    "all_metrics.append((llm_acc, llm_prec, llm_f1))\n",
    "all_cms.append(llm_cm)\n",
    "print(\"\\nLLM Results:\")\n",
    "print(f\"Accuracy: {llm_acc:.2f}\")\n",
    "print(f\"Precision: {llm_prec:.2f}\")\n",
    "print(f\"F1: {llm_f1:.2f}\")\n",
    "print(\"Confusion Matrix:\\n\", llm_cm)\n",
    "\n",
    "# RAC\n",
    "rac_preds = [rac.predict(text) for text in test_texts]\n",
    "all_preds.append(rac_preds)\n",
    "rac_acc = accuracy_score(true_labels, rac_preds)\n",
    "rac_prec = precision_score(true_labels, rac_preds, average='macro', zero_division=0)\n",
    "rac_f1 = f1_score(true_labels, rac_preds, average='macro', zero_division=0)\n",
    "rac_cm = confusion_matrix(true_labels, rac_preds)\n",
    "all_metrics.append((rac_acc, rac_prec, rac_f1))\n",
    "all_cms.append(rac_cm)\n",
    "print(\"\\nRAC Results:\")\n",
    "print(f\"Accuracy: {rac_acc:.2f}\")\n",
    "print(f\"Precision: {rac_prec:.2f}\")\n",
    "print(f\"F1: {rac_f1:.2f}\")\n",
    "print(\"Confusion Matrix:\\n\", rac_cm)\n",
    "\n",
    "# --- Visualization ---\n",
    "for i, (cm, method) in enumerate(zip(all_cms, methods)):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix: {method}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bbd5e14-a309-406f-a29b-acf099d01f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmdir: /s: No such file or directory\n",
      "rmdir: /q: No such file or directory\n",
      "rmdir: chroma_db: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "rmdir /s /q chroma_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e10f526f-c999-4edc-898f-73829bc9873b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset (assume it's a CSV file)\n",
    "df = pd.read_csv(\"DBPEDIA_test 5.csv\")  # replace with your actual filename\n",
    "\n",
    "# Count category frequency\n",
    "top_categories = df['l3'].value_counts().nlargest(100).index\n",
    "\n",
    "# Filter the dataset to keep only rows with top 100 categories\n",
    "subset_100_df = df[df['l3'].isin(top_categories)]\n",
    "\n",
    "# Save the smaller subset to a new file (optional)\n",
    "subset_100_df.to_csv(\"top_100_categories_subset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e55f60a-02e1-4c48-ab29-c06da8f812c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset (assume it's a CSV file)\n",
    "df = pd.read_csv(\"DBPEDIA_test 5.csv\")  # replace with your actual filename\n",
    "\n",
    "# Count category frequency\n",
    "top_categories = df['l3'].value_counts().nlargest(50).index\n",
    "\n",
    "# Filter the dataset to keep only rows with top 100 categories\n",
    "subset_50_df = df[df['l3'].isin(top_categories)]\n",
    "\n",
    "# Save the smaller subset to a new file (optional)\n",
    "subset_50_df.to_csv(\"top_50_categories_subset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0bd69a27-eb37-499b-8630-740144aeb109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset (assume it's a CSV file)\n",
    "df = pd.read_csv(\"DBPEDIA_test 5.csv\")  # replace with your actual filename\n",
    "\n",
    "# Count category frequency\n",
    "top_categories = df['l3'].value_counts().nlargest(20).index\n",
    "\n",
    "# Filter the dataset to keep only rows with top 100 categories\n",
    "subset_20_df = df[df['l3'].isin(top_categories)]\n",
    "\n",
    "# Save the smaller subset to a new file (optional)\n",
    "subset_20_df.to_csv(\"top_20_categories_subset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62da8d1b-a389-4323-90f9-89c7b288c3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 45768 entries, 2 to 60792\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    45768 non-null  object\n",
      " 1   l1      45768 non-null  object\n",
      " 2   l2      45768 non-null  object\n",
      " 3   l3      45768 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "subset_100_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90303f33-d42f-4b7c-be05-325090a1c5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 24106 entries, 2 to 60792\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    24106 non-null  object\n",
      " 1   l1      24106 non-null  object\n",
      " 2   l2      24106 non-null  object\n",
      " 3   l3      24106 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 941.6+ KB\n"
     ]
    }
   ],
   "source": [
    "subset_50_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d28246fe-7343-4f10-a93f-ea8df09b505d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9679 entries, 2 to 60768\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    9679 non-null   object\n",
      " 1   l1      9679 non-null   object\n",
      " 2   l2      9679 non-null   object\n",
      " 3   l3      9679 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 378.1+ KB\n"
     ]
    }
   ],
   "source": [
    "subset_20_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cb5ba8-8544-4e1f-9ac5-48562f7f7d8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
